#### 集成算法

集成学习根据各个弱分类器之间有无依赖关系，分为Boosting和Bagging两大流派：

Boosting流派，各分类器之间有依赖关系，必须串行，比如Adaboost、GBDT(Gradient Boosting Decision Tree)、Xgboost

Bagging流派，各分类器之间没有依赖关系，可各自并行，比如随机森林（Random Forest）

### Boosting

<img src="https://i.loli.net/2020/03/01/X2cpQBqnser1Zjd.png" alt="2.png" style="zoom: 67%;" />

### Bagging

<img src="https://i.loli.net/2020/03/01/o1Xz9c6IuJKNaTO.png" alt="bagging.png" style="zoom:67%;" />

### Stacking

<img src="https://i.loli.net/2020/03/01/eKXxf8BnNFqu5Cy.png" alt="3.png" style="zoom:67%;" />

#### 提升树模型boosting tree

- 最小二乘回归树生成算法：用于生成树模型（将数据集根据最佳划分，生成回归树）

  > 这里每一次划分一个区域，都是要计算这个区域整个的每一个特征下的每一个输入值作为划分点，计算出整个这个区域最小的均方损失值，这样最小的（j,s)组合就是该区域最佳的区域划分点。

  ![最小二乘回归树生成算法.png](https://i.loli.net/2020/03/01/JVny4CiapdP2oBG.png)

- 提升树：采用前向分布算法和加法模型实现。先根据最小二乘回归树生成算法生成一棵树，然后计算每个区域的残差（残差=y - 区域的均值），将这个树的残差作为下一棵树的输入值，依次循环，知道损失值达到阈值或者达到生成树的个数，停止循环，那么最后一次的树，就是这次回归最好的树。
  $$
  \Theta_{m}=\arg \min _{\Theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \Theta_{m}\right)\right)
  $$
  ​	这个公式是为了找到本轮迭代的决策树，是的样本的损失尽量变得更小。

  ![bb.png](https://i.loli.net/2020/03/01/thLOo2FdA46MvPK.png)

  ​	![bb2.png](https://i.loli.net/2020/03/01/7ERlTxsAyvQzet1.png)

  > 提升树利用前向分布算法和加法模型实现学习的优化过程，但是一般的损失函数的每一步优化不是那么容易（上面的损失函数采用的均方差误差，得到了下一棵树需要拟合的值是上一颗数的残差，其他的损失函数就不能得到残差去的倒下一棵树）。
  >
  > **总结**：提升树，利用残差进行学习

#### 梯度提升树GBDT 

**利用损失函数的负梯度拟合本轮损失的近似值**，从而拟合一个cart回归树。

![gbdt1.png](https://i.loli.net/2020/03/01/WKjh9vS13DLNgmA.png)

​	通过计算每一个样本的负梯度（这和区域的均值是一样的），进而计算残差，将这个残差作为下一棵树的输入，循环直到结束。最后得到强回归器。

- GBDT的回归算法

![gbdt2.png](https://i.loli.net/2020/03/01/EFj8uw3yGV7lsKf.png)

- GBDT 的分类算法：采用指数损失函数或对数似然函数

  - 二元分类：采用指数函数损失
    ![gbdt3.png](https://i.loli.net/2020/03/01/hPwIfM51C42ijKu.png)

  - 多元分类：采用对数函数损失

    > - 常用的分类损失函数：
    >
    >   - 指数损失函数：
    >
    >   $$
    >   L(y, f(x))=\exp (-y f(x))
    >   $$
    >
    >   - 对数损失函数：
    >
    >   $$
    >   L(y, f(x))=\log (1+\exp (-y f(x)))
    >   $$

    > - 常用的回归函数：
    >
    >   - 均方差损失
    >
    >   - 绝对值损失
    >
    >   - Huber损失函数：他是均方差和绝对损失的折中产物，对于远离中心的异常点，采用绝对损失，而离中心附近的点采用均方差，这个界限一般用分位数点度量。δ的选择非常关键，因为它决定了你如何看待异常值。残差大于δ，就用L1（它对很大的异常值敏感性较差）最小化，而残差小于δ，就用L2“适当地”最小化
    >     $$
    >     L_{\delta}(y, f(x))=\left\{\begin{array}{ll}
    >     \frac{1}{2}(y-f(x))^{2} & \text { for }|y-f(x)| \leq \delta \\
    >     \delta|y-f(x)|-\frac{1}{2} \delta^{2} & \text { otherwise }
    >     \end{array}\right.
    >     $$

  - GBDT主要的优点有：

    　　　　(1) 可以灵活处理各种类型的数据，包括连续值和离散值。

    　　　　(2) 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。

    　　　　(3）使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。

  - GBDT的主要缺点有：由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来 达到部分并行。

    <hr>

#### Xgboost

$$
L(\phi)=\sum_{i} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)
$$

$$
L(\phi)=\sum_{i} l\left(y_{i}, \hat{y}_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)
$$

$$
\Omega\left(f_{k}\right)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}
$$

- $$
  \Omega\left(f_{k}\right)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}
  $$

  ![6.png](https://i.loli.net/2020/03/01/LSm7j9CoZEl6fvB.png)

- xgboost和GBDT有什么区别除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。

  1. GBDT是机器学习算法，XGBoost是该算法的工程实现。
  2. 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
  3. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
  4. 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。
  5. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。
  6. 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。
  7. CART回归树中寻找最佳分割点的衡量标准是**最小化均方差**，xgboost寻找分割点的标准是**最大化增益**，lamda，gama与正则化项相关，
     ![5.png](https://i.loli.net/2020/03/01/G3fIWOku8TN1EUw.png)

- 为什么xgboost要用泰勒张开，优势在哪里？

  - XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。

- 优缺点：优缺点：

  - 在寻找最佳分割点时，考虑传统的枚举每个特征的所有可能分割点的贪心法效率太低，xgboost实现了一种近似的算法。大致的思想是根据百分位法列举几个可能成为分割点的候选者，然后从候选者中根据上面求分割点的公式计算找出最佳的分割点。
  - xgboost考虑了训练数据为稀疏值的情况，可以为缺失值或者指定的值指定分支的默认方向，这能大大提升算法的效率，paper提到50倍。
  - 特征列排序后以块的形式存储在内存中，在迭代中可以重复使用；虽然boosting算法迭代必须串行，但是在处理每个特征列时可以做到并行。
  - 按照特征列方式存储能优化寻找最佳的分割点，但是当以行计算梯度数据时会导致内存的不连续访问，严重时会导致cache miss，降低算法效率。paper中提到，可先将数据收集到线程内部的buffer，然后再计算，提高算法的效率。
  - xgboost 还考虑了当数据量比较大，内	存不够时怎么有效的使用磁盘，主要是结合多线程、数据压缩、分片的方法，尽可能的提高算法的效率
    

####  自适应提升Adaboost

- 它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。
- 步骤
  - 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
  - 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
  - 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。

#### 随机森林RF

优点：

1）表现性能好，与其他算法相比有着很大优势。
2）随机森林能处理很高维度的数据（也就是很多特征的数据），并且不用做特征选择。
3）在训练完之后，随机森林能给出哪些特征比较重要。
4）训练速度快，容易做成并行化方法(训练时，树与树之间是相互独立的)。
5）在训练过程中，能够检测到feature之间的影响。
6）对于不平衡数据集来说，随机森林可以平衡误差。当存在分类不平衡的情况时，随机森林能提供平衡数据集误差的有效方法。
7）如果有很大一部分的特征遗失，用RF算法仍然可以维持准确度。
8）随机森林算法有很强的抗干扰能力（具体体现在6,7点）。所以当数据存在大量的数据缺失，用RF也是不错的。
9）随机森林抗过拟合能力比较强（虽然理论上说随机森林不会产生过拟合现象，但是在现实中噪声是不能忽略的，增加树虽然能够减小过拟合，但没有办法完全消除过拟合，无论怎么增加树都不行，再说树的数目也不可能无限增加的）。
10）随机森林能够解决分类与回归两种类型的问题，并在这两方面都有相当好的估计表现。（虽然RF能做回归问题，但通常都用RF来解决分类问题）。
11）在创建随机森林时候，对generlization error(泛化误差)使用的是无偏估计模型，泛化能力强。
缺点：

1）随机森林在解决回归问题时，并没有像它在分类中表现的那么好，这是因为它并不能给出一个连续的输出。当进行回归时，随机森林不能够做出超越训练集数据范围的预测，这可能导致在某些特定噪声的数据进行建模时出现过度拟合。（PS:随机森林已经被证明在某些噪音较大的分类或者回归问题上回过拟合）。2）对于许多统计建模者来说，随机森林给人的感觉就像一个黑盒子，你无法控制模型内部的运行。只能在不同的参数和随机种子之间进行尝试。3）可能有很多相似的决策树，掩盖了真实的结果。4）对于小数据或者低维数据（特征较少的数据），可能不能产生很好的分类。（处理高维数据，处理特征遗失数据，处理不平衡数据是随机森林的长处）。5）执行数据虽然比boosting等快（随机森林属于bagging），但比单只决策树慢多了。

适用场景：数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

#### 逻辑回归LR

- 优点：实现简单，广泛的应用于工业问题上；分类时计算量非常小，速度很快，存储资源低；便利的观测样本概率分数；对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

- 缺点：当特征空间很大时，逻辑回归的性能不是很好；容易欠拟合，一般准确度不太高
  - 不能很好地处理大量多类特征或变量；只能处理两分类问题（在此基础上衍生出来的softmax可以用于多分类），且必须线性可分；对于非线性特征，需要进行转换。

  - 适用场景：LR同样是很多分类算法的基础组件，它的好处是输出值自然地落在0到1之间，并且有概率意义。因为它本质上是一个线性的分类器，所以处理不好特征之间相关的情况。虽然效果一般，却胜在模型清晰，背后的概率学经得住推敲。它拟合出来的参数就代表了每一个特征(feature)对结果的影响。也是一个理解数据的好工具。
    

