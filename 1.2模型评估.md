 #### 模型评估

- 分类问题：
  - 混淆矩阵：主要用于比较分类结果和实例的真实信息。矩阵中的每一行代表实例的预测类别，每一列代表实例的真实类别。
    ![1.png](https://i.loli.net/2020/02/18/DlwzHZuj1BXLNph.png)

> - 真正True Positive：被预测为正类的正样本
>
> - 假正False Positive:被预测为正类的负样本
>
> - 假负False Negative：被预测为负的正样本
>
> - 真负True Negative：被预测为负法人负样本
>
>   
>
> - 准确率：正确预测的正反例数/总数 Accuracy=(TP+TN)/(TP+FP+FN+TN)
>
> - 精准率：针对预测正确的正样本而不是所有预测正确的样本。表现为预测出的正的里面的真正的正的有多少，也就收查准率Precision=TP/(TP+FP)
>
> - 召回率：表现出在实际正样本中，分类器能预测出多少，也就是查全率。Recall=TP/(TP+FN)
>
> - F1 score:是精准率和召回率的调和值，更接近于两个数较小的那个，所以Precision和Recall 接近时，F值最大。2/F1=1/Precision+1/Recall
>
> - ROC曲线：横坐标是FPR假正率，纵坐标是TPR真正率，这个曲线应该都处于（0,0），（1,1）连线的上方。
>   ![ROC.png](https://i.loli.net/2020/02/18/CVAdv59kLbpQFSy.png)
>
>   ROC曲线中的四个点和一条线:
>   点(0,1)：即FPR=0, TPR=1，意味着FN＝0且FP＝0，将所有的样本都正确分类。
>   点(1,0)：即FPR=1，TPR=0，最差分类器，避开了所有正确答案。
>   点(0,0)：即FPR=TPR=0，FP＝TP＝0，分类器把每个实例都预测为负类。
>   点(1,1)：分类器把每个实例都预测为正类。
>   总之：ROC曲线越接近左上角，该分类器的性能越好。而且一般来说，如果ROC是光滑的，那么基本可以判断没有太大的overfitting
>
> - AUC：被定义为ROC曲线下的面积，通常大于0.5小于1.AUC值(面积)越大的分类器，性能越好，
>   ![2.png](https://i.loli.net/2020/02/18/CRSchPWt9wYOFpK.png)
>
> - PR曲线：横坐标是精准率P，纵坐标是召回率R，PR曲线的横坐标是精确率P，纵坐标是召回率R。评价标准和ROC一样，先看平滑不平滑（蓝线明显好些）。一般来说，在同一测试集，上面的比下面的好（绿线比红线好）。当P和R的值接近时，F1值最大，此时画连接(0,0)和(1,1)的线，线和PRC重合的地方的F1是这条线最大的F1（光滑的情况下），此时的F1对于PRC就好像AUC对于ROC一样。一个数字比一条线更方便调型。
>   ![3.png](https://i.loli.net/2020/02/18/jyJkPbWhUKqr21N.png)

- 回归问题：
  - 平均绝对误差MAE :
  - 平均平方误差MSE：
  - 均方根误差RMSE：