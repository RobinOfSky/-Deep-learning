### 特征工程篇

![tzgc.png](https://i.loli.net/2020/02/18/WxKRskvliwUJPeG.png)

> 数据和特征决定了机器学习的上限，而算法和模型只是毕竟这个上限而已

1. 入门概念

- 模型：描述数据不同部分之间的关系

- 特征：原始数据的数值表示。特征有很多种类型。

  > **选择特征**：什么样的特征适合某一种算法，选择对算法有帮助的特征，显得尤为的重要。
  >
  > - 想特征需要不少的领域知识，现在还没有一套想特征的办法，只能是猜想一些对某一问题相关的特征，来验证特征是否对算法有用，来筛选特征。
  > - **总结**：选特征的流程，就是先猜想，然后统计验证，然后将特征加到模型中，进行验证
  >
  > **验证特征**：
  >
  > - 直接观察模型的输出结果
  > - 卡方检验
  > - 单特征AUC

- 特征工程；在给定数据，模型和任务的情况下设计出最合适的特征的过程。

  > 特征的数量：既不能太少也不能太多；
  >
  > - 特征太少：模型将不能表现的太好
  > - 特征太多：可能有些特征对模型的影响可有可无，那么这些特征会使得模型的训练时间很长，训练的泛化能力不是太好。

> 问题：机器学习和深度学习的区别和联系？
>
> - 机器学习是通过人为的根据数据的一些属性手工设置特征，在通过回归器，分类器得到结果。
> - 深度学习是通过网络来学习数据具有的特征，进而得出结果
> - 联系：深度学习也是机器学习的一部分，只不过处理的学习特征的方式不同。



2. 标量，向量和空间

- 标量：单独的数值性特征
- 向量：标量的有序列表
- 空间：不同的特征组成的区域
  - 特征空间中的数据：由特征构成的区域来表示数据
  - 数据空间站的特征：由数据作为维度，来表示特征

3. 特征的二值化和图像的二值化

- 对特征进行二值化：这种处理主要是对特征不在意特征值的大小，而在意是否特征值的存在，也就是有无，有得话，就是1，没有的话就是0，不管特征值的值有多大，多小，特征二值化之后后会成为这样的[0,1].（也可以设置阈值，大于阈值的为1，小于阈值的为0）

  ```python
  ### skleran 
  from skleran.preprocessing import Binarizer  
  BInarizer(threshold=4).fit-transform(iris.data)
  ```

  

- 对图像进行二值化：图像二值化一般使得图像的像素单一，图像更简单，。阈值可分为全局阈值和局部与之，可以使单阈值也可以是多阈值。图像的二值化，主要是为了提取感兴趣的像素分离处理作为前景像素，不感兴趣的作为背景像素。

  ```opencv
  retval,dst=cv2.threshold(src,thresh,maxval,type)
  #type表示这里划分的时候使用的是什么类型的算法cv2.THRESH_BINARY（黑白二值）
    #cv2.THRESH_BINARY_INV（黑白二值反转）
    #cv2.THRESH_TRUNC （得到的图像为多像素值）
    #cv2.THRESH_TOZERO
    #cv2.THRESH_TOZERO_INV`
  ```

- **特征的处理**：对特征要进行一些处理，为的使算法更加的鲁棒。

  - 单个特征：

    - 二值化：如果只关心特征的有无，不关系具体的特征值的大小，可以采用这种阈值的方式进行二值化。这样处理会使得计算速度会更加的快。

    - 归一化（特征缩放）：如果模型收输入尺度的影响就需要对其归一化，也就是输入数据的一组尺度相差很大时，就需要进行特征缩放。如果有尺度相差很大的数据，不进行特征缩放的话，会导致算法训练的不稳定。

      - 标准化归一化：减去均值除以方差，使得数据服从正态分布。
      - 最大最小归一化：减去最小值除以极差，使得数据分布为[0,1]之间

    - 离散化：数据离散化是指将连续的数据进行分段，使其变为一段离散化的区间。分段的原则有基于等距离，等频率或优化的方法。

      - 算法的需要：比如逻辑回归，决策树，朴素贝叶斯算法，都是基于离散数据展开的。使用该类算法，必须要将数据离散化。有效的离散化能减小算法的时间和空间的开销，提高系统对样本的分类能力和抗噪声能力。
      - 离散化的特征相对于连续型特征更易理解，更接近知识层面的表达
      - 可以有效的克服数据中的隐藏缺陷，是模型结果更加稳定
      - 引入非线性特征，很方便做特征交叉

    - 特征交叉：引入特征之间的交互，为了引入非线性，

    - 缺失值处理：
      ![缺失值的处理.png](https://i.loli.net/2020/02/18/WA1luECeR7rkh9D.png)

      - 缺失值的类型：
        - 完全随机缺失：指的是数据的确实是完全随机的，不依赖任何不完全变量或完全变量，不影响样本的无偏性。
        - 随机缺失：指的是数据的确实不是随机的，及该类数据的确实依赖于其他的完全变量。
        - 非随机缺失：指的是数据的确实与不完全变量自身的取值有关。

        > 对于随机缺失和非随机缺失，直接删除记录是不合适的，随机缺失可以通过已知变量对缺失值进行估计，而且非随机的非随机还没有很好的解决办法。

      - 缺失值的处理：

        - 连续型：
          - 直接删除
          - 插补（适用于缺失值少）
          - 拟合（适用于缺失值多）
          - 衍生（适用于缺失值多）
        - 类别型：
          - 缺失值少：
          - 缺失值多：
          - 缺失值适中：

    - 异常值分析：

      - 查看异常值情况：
      - 异常值的处理：

  - 多个特征：

    - 选取准则：

      - 特征是否发散：如果一个特征不发散，那么这个特征基本上没有差异，这个特征对于样本的区分并没有什么用，
      - 特征与目标的相关性：选取优先相关的特征

    - 方法：

      - filter：按照发散性和相关性对各个特征进行评分，设定阈值或者阈值的个数，选择特征

        - 方差选择法：先计算各个特征的方差，选择方差大于阈值的特征VarianceThreshold

          ```python
          from sklearn.feature_selection import VarianceThreshold
          
          # 方差选择法，返回值为特征选择后的数据
          # 参数threshold为方差的阈值
          VarianceThreshold(threshold=3).fit_transform(iris.data)
          ```

        - 相关系数法：先要计算各个特征对目标值的相关系数的P值，SelectKBest

          ```python
          from sklearn.feature_selection import SelectKBest
          from scipy.stats import pearsonr
          
          #选择K个最好的特征，返回选择特征后的数据
          #第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
          #参数k为选择的特征个数
          SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
          # 此处代码有问题，得改
          ```

        - 卡方检验：检验定性自变量对定性因变量的相关性，chi2

          ```python
          from sklearn.feature_selection import SelectKBest
          from sklearn.feature_selection import chi2
          
          # 选择K个最好的特征，返回选择特征后的数据
          SelectKBest(chi2,k=2).fit_transform(iris.data,iris.target)
          ```

        - 互信息法：评价定性自变量对定性应变量的相关性：

          ```python
          from sklearn.feature_selection import SelectKBest
          from minepy import MINE
          
          # 由于MINE的设计不是函数式，定义mic方法将其改为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
          def mic(x,y):
              m = MINE()
              m.compute_score(x,y)
              return (m.mic(),0.5)
          
          # 选择K个最好的特征，返回特征选择后的数据
          SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
          # 此处代码有问题，得改
          ```

      - wrapper:根据目标函数，每次选择若干特征，或者是排除若干特征

        - 递归特征消除法：使用一个基模型来进行多轮训练，每轮训练完，消除若干权值系数的特征，再给予新的特征集进行下一轮训练RFE

          ```python
          from sklearn.feature_selection import RFE
          from sklearn.linear_model import LogisticRegression
          
          # 递归特征消除法，返回特征选择后的数据
          # 参数estimator为基模型
          # 参数n_features_to_select为选择的特征个数
          RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(iris.data,iris.target)
          ```

      - embedded：先试用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从小到大选择特征

        - 基于惩罚项 的特征选择：处理筛选出特征外，同时也进行了降维

          ```python
          from sklearn.feature_selection import RFE
          from sklearn.linear_model import LogisticRegression
          
          # 递归特征消除法，返回特征选择后的数据
          # 参数estimator为基模型
          # 参数n_features_to_select为选择的特征个数
          RFE(estimator=LogisticRegression(),n_features_to_select=2).fit_transform(iris.data,iris.target)
          # 此处代码有问题
          ```

        - 给予树模型的特征选法：树模型GBDT也可以作为基模型来进行特征选择SelectFromModel

          ```css
          from sklearn.feature_selection import SelectFromModel
          from sklearn.ensemble import GradientBoostingClassifier
          
          # GBDT作为基模型的特征选择
          SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data,iris.target)
          ```

- 降维：PCA主成分分析，LDA线性判别模型，线性判别模型分析本身也是一个分类模型。PCAheadLDA有很多的相似点，其主要是将原始样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能。所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

  - PCA：

    ```python
    from sklearn.decomposition import PCA
    
    # 主成分分析法，返回降维后的数据
    # 参数n_components为主成分数目
    PCA(n_components=2).fit_transform(iris.data)
    ```

  - LDA：

    ```python
    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
    
    # 线性判别分析法，返回降维后的数据
    # 参数n_components为降维后的维数
    LDA(n_components=2).fit_transform(iris.data,iris.target)
    ```

> 什么算法的输入是离散特征，什么算法的输入是连续特征？

- 分类变量的编码：分类变量中的类别通常不是数值类型的，因此需要将其分类类别转换为数值
  - one-hot编码：每一个比特位表示一种特征
  - 虚拟编码：
  - 效果编码：

- 对数变化：
  数据分布的倾斜有很多负面的影响，可以使用特征工程技巧，减轻数据分布倾斜影响，使原本密集区域的值尽可能的分散，原本分散的区域尽量聚合。log变换主要作用帮助稳定方差，始终保持分布接近正态分布并使得数据与分布的平均值无关。
  $$
  y=\log _{c}(1+\lambda x)
  $$

λ通常设置为1，c通常设置为对数变换的最大值，使得倾斜分布尽可能的接近正态分布。

> 使得数据服从正态分布，这样训练效果会更好。