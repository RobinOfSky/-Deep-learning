#### 损失函数，代价函数，目标函数

- 概念：
  - 损失函数：是定义在单个样本上的，是指一个样本的误差
  - 代价函数：是定义在整个训练集上的，是所有样本误差的平均，也就是所有损失函数值的平均
  - 目标函数：是指最终优化的函数，一般是经验风险+结构风险，也就是代价函数+正则化
- 常用损失函数：
  - 0-1损失：预测值和真实值不相等时，损失为1，相等时为0.
  - 平方损失函数：真实值与预测值差的平方
  - 绝对损失：真实值与预测值的差的绝对值
  - 对数损失：-log p(y|x) ：x是样本，y是预测值，p(y|x)是预测正确的概率，由于概率之间同时满足需要使用乘法，为了将其转化为加法，我们将其取对数，最后由于是损失函数，所以预测正确的概率越高，其损失值应该越小，因此价格负号取反。
  - hinge loss:一般是分类算法中的损失函数，尤其是SVM，
    - 二分类：Loss=max{0,1-y*f(x)}
    - 多分类：Loss=max{0,1 + max{分类错误的值取最大值} -  被正确分类的值}
- 常用代价函数：
  - 回归：
    - 平均绝对误差：MAE， 平均绝对误差是绝对误差的平均值 ，平均绝对误差能更好地反映预测值误差的实际情况。 **通常用来作为回归算法的性能指标**。
    - 均方误差：MSE，均方误差是指参数估计值与参数真值之差平方的期望值; MSE可以评价数据的变化程度，MSE的值越小，说明预测模型描述实验数据具有更好的精确度。**通常用来做回归问题的代价函数**。
    - 均方根误差：RMSE，在MSE的基础上，取sqrt
  - 分类：
    - 交叉熵代价函数：![1.png](https://i.loli.net/2020/02/21/ikqc5ze3nrQdyMH.png)
      交叉熵是用来评估当前训练得到的概率分布于真实分布的差异情况，减少交叉熵损失就是在提高模型的预测准确率，p(x)指的是真实分布的概率，q(x)指的是通过模型计算出来的概率估计

