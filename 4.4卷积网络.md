#### 卷积网络

- 相比于全连接网络，卷积网络为什么可以更好的处理图像数据？

  - 局部感受野：卷积核作用的那个区域
  - 权值共享

- 卷积后特征图大小的计算：有两种方式，一种是VALID和SAME，卷积核池化都适用，**卷积除不尽的结果都向下取整，池化除不尽的结果都向上取整**

  > 输入特征图的大小s_in x s_in,输出特征图的大小为s_out x s_out,卷积核的大小为kernel_size，填充为padding，卷积和池化步长为stride

  - VALID：，不足的尺寸将会丢弃。s_out = (s_in - Kernel_size) / stride + 1

  - SAME:这种方式计算的特征图的大小与输入特征图的大小保持一致。s_out = (s_in + 2 x padding - kernel_size) / stride +1

    > 卷积核的大小一般和padding有关：如果卷积核的大小是1x1，那么padding为0；如果卷积核的大小是3x3，那么padding为1；如果卷积核的大小是5x5，那么padding为2

- 池化后后特征图大小的计算：和Valid的计算方式一样
- 多个小卷积核连续卷积和单个大的卷积核卷积的效果作用相同。（一个5x5=2 个 3x3的）：使用3x3的卷积核连续卷积两次可以达的5x5卷积核卷积一次提取特征图的能力。
- 小卷积核的优势：
  - 整合了三个非线性激活层，代替了单一非线性激活层，增加了判别能力。
  - 减少了参数
  - 减少了计算量

- 转置卷积：先对原始特征矩阵进行填充使其维度扩大倒是佩娟及目标输出维度，然后就行普通的卷积操作。但这个操作不是真正的逆变换操作，转置卷积常用于目标检测领域对小目标的检测和图像分割领域还原输入图像尺度。
- 1x1卷积的作用：
  - 整合通道的特征信息
  - 减少卷积核的通道数，减小运算量