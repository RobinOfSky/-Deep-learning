#### 激活函数

- 激活函数应该具有的性质：
  - 非线性，线性激活层对于深层神经网络没有作用，因为其作用以后仍然是输入的各种线性变换
  - 连续可微：梯度下降的要求
  - 范围最好不饱和，也就是梯度一直大于0，如果梯度等于0的话，那么就会是的训练停止
  - 单调性：单层神经网络的误差函数是凸的，好优化
  - 在原点处近似线性，这样当权值初始化为接近0的随机值时，网络可以学习的较快，不用调节网络的输出值

- sigmoid：

  - 缺点：

    - 饱和时梯度值非常小：由于反向传播梯度时，后面的梯度是以乘的方式传递到前层，因此当层数比较多的时候，传到前层的梯度会非常的小，权值得不到有效的更新，即梯度弥散。如果该层的权值初始化使得激活函数处于饱和状态，网络基本上权值无法更新。

      <img src="https://i.loli.net/2020/02/22/unwDZe4MK385hUT.png" alt="1.png" style="zoom:50%;" />上图是sigmoid的导数的图像，可见取值范围为[-0.25 , 0].也就是每次反向传播的时候都会乘以一个小于1的负数，所以会导致梯度减小。

    - 输出值不是以0为中心值

  - tanh：仍然具有饱和问题

  - relu：解决了梯度弥散的问题

    - 优点：	
      - x>0时，梯度恒为1，无梯度弥散问题，收敛快；当x < 0,增大了网络的稀疏性,训练完成之后为0的神经元越多，稀疏性越大，提取出来的特征就越具有代表性，泛化能力越强，真正起作用的神经元越少，网络的泛化能力就越好
      - 运算量小
    - 缺点：
      - 输入值小于0，输出值就会是0；一阶导也是0.这样导致训练开始时一些神经元就是死的。