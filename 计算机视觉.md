### 计算机视觉

#### 图像分类

- LeNet,AlexNet,ZFNet,Googlenet,VGGNet,ResNet,DenseNet，Inception V3

  ------

  

#### 目标检测：在图像中对可变数量的目标进行查找和分类（不同物体共有的属性 物体大小的检测，物体属性的检测）

- 传统目标检测和深度学习目标检测：
  - 传统目标检测：Viola-Jones，HOG+SVM，DPM
    - 手动设计特征
    - 滑动窗口(缺点：
    - 传统的分类器
    - 多步骤
    - 准确度和实时性差
  - 深度学习目标检测：one-stage,two-stage
    - 深度学习网络学习特征
    - proposal或者直接回归
    - 深度网络
    - 端到端
    - 准确度和实施性高

- HOG ：histograms of oriented gradients 行人检测，在目标检测领域，单一特征效果中效果比较好，HOG特征还被应用到姿态估计，人脸识别，表情识别，场景分类等。

  - HOG的流程：

    - 输入图像，伽马色彩校正，计算梯度，统计梯度方向，归一化，向量化

      <img src="https://i.loli.net/2020/01/30/o7rvsfSnexIiG5O.png" alt="081701052124062.png" style="zoom: 67%;" />

      

      - 伽马校正：使用伽马变换（在图像处理中，将漂白也就是相机过度曝光的图片或者是过暗也就是曝光不足的图片进行修正）对图像预处理，增强图像俺去的对比度
        <img src="https://i.loli.net/2020/01/30/4Zwjp3sE6AQ8kdc.png" alt="伽马.png" style="zoom:50%;" />

      - gamma值小于1时，会拉伸图像灰度级较低的区域，同时会压缩灰度级较高的部分（可以用来处理图像的对比度）
      - gamma值大于1时，会拉伸图像灰度级较高的区域，同时压缩灰度级较低的部分（可以用来处理较暗的图像）

    - 计算梯度：通过计算水平方向和竖直方向的梯度，在求得改点处的梯度和梯度的方向，实际计算时，分别使用模板[-1,0,1]和[-1,0,1]的转置对图像卷积得到水平和竖直方向的梯度，对于多通道图像，选择梯度幅值最大的通道的梯度作为该像素点的梯度。

      > **插值**：也叫内插，是一种通过已知，离散的数据点，在范围内推求新数据点的过程或方法。常见的三种插值：最近领域插值，双线性插值，双三次插值。也可以这样理解：在一个函数里面，自变量是离散有间隔的，插值就是我那个自变量的建个之间插入行的自变量，然后求解新的自变量函数值。
      >
      > - 在图像处理体现在图像的缩放上面，如放大图像 ，在像素点的层面上其实就是往像素点之间插入行的像素点，从而增大图像的分辨率
      > - 最近领域插值：选取插入点最近点的像素灰度值为要插入像素点的灰度值，距离的话可以使欧氏距离或者其他
      > - 线性插值：
      >   <img src="https://i.loli.net/2020/01/30/425xDgRI67WaUMO.png" alt="线性.png" style="zoom: 50%;" />

    - 统计梯度方向：这一步骤是HOG特征计算的核心，将图像划分为的单元（Cell），个单元组成一个块（Block），块内的梯度插值和加权方式如图 2.9所示。块内的像素点对块内的单元进行投票。每个单元都统计一个梯度方向直方图。梯度方向没有符号区分，即将梯度方向在范围内等间隔划分为9个区间（Bin）。每一个像素点的梯度方向都采用线性插值，即对梯度方向相邻的两个区间投票。此外，还要对该像素点周围的单元进行双线性插值投票，具体来说，F、G、J、K内的像素点需要对Cell 0-Cell 3共4个单元进行插值，而A、D、P、M内的点只需对其所在的单元插值，其余的区域需要对2个单元插值。像素点对某一个单元的某一个梯度方向的权重，为该点的梯度方向在该梯度方向和该单元插值后，再经过高斯加权后的值。如图 2.9所示，高斯加权使得越靠近块中心的像素点权值越大，反之越小。
      
      <img src="https://i.loli.net/2020/01/30/TzSIi1xAaVe2jWG.png" alt="block.png" style="zoom:80%;" />
      
    - 归一化：为了克服关照不均匀以及前景与背景的对比差异，有必要采用局部归一化
      
    - 向量化：将检测窗口内的特征组成一维向量

      > - 无符号梯度：梯度的方向是0到180度
      >
      > - 有符号梯度：0到360度

      > - overlap:分割出的区块互相交叠，有重合的区域
      >
      > - non-overlap：没有重合的区域。

  - DPM目标检测算法：是一种基于部件的检测方法，对目标的形变具有很强的鲁棒性。采用改进后的HOG特征，SVM分类器和滑动窗口检测思想，正对目标的多视角问题，采用多组件的策略；正对目标本身的形变问题，采用基于图结构的部件模型策略（使用一个滑动窗口在整个图片上均匀华东，用分类器评估是否有物体）

  > 问题：
  >
  > - 目标种类与数量问题
  > - 目标尺度问题
  > - 外在环境干扰问题

- 主流的算法分为两个类型：
  <img src="https://i.loli.net/2020/01/30/3kCFP9D5EOGd2JL.png" alt="搜狗截图20200130111109.png" style="zoom: 50%;" />
  

![ab0bvqb6cu.jpeg](https://i.loli.net/2020/01/30/OxmdrT1KQRsU86t.jpg)

  - two-stage:R-CNN系列算法，通过采取selective search和RPN网络生成候选框，然后对这些候选框进行分类和回归，准确率高

  - one-stage:YOLO，SSD，在图片的不同位置进行密集采样，抽样时采取不同的尺寸和长宽比，然后利用CNN提取特征后进行分类和回归，整个过程只需要一步，搜易有事是速度快，但是均匀的密集采样的一个缺点是训练比较困难，这个主要是正样本和负样本机器不均匀，单只模型准确的稍低。
  
- 传统目标检测算法：
  - （穷举策略：采用滑动窗口，且设置不同的大小，不同的长宽比对图像进行遍历，时间复杂度高）
  - 特征提取（SIFT，HOG等多样化，光照多样性，背景多样性使得特征鲁棒性差）
  - 分类器分类（主要SVM，Adaboost）

- RCNN:区域卷积神经网络。(Selective Search + CNN + SVM)
  ![RCNN.png](https://i.loli.net/2020/03/08/8tXPqGYkslxHZTr.png)

  - 简略步骤：

    - 将给定的输入图片，从中提取2000个类别独立的获选区域
    - 再对每一个区域利用CNN抽取一个固定长度的特征向量
    - 再对每一个区域进行回归（调整候选区域的位置和大小------>IOU）和SVM（判断候选框里面的物体类别）的目标分类

  - 候选区域：生成候选区域的方法有很多种，RCNN采用的是Selective Search。非极大值抑制，选出置信度最高的候选框，如果和当前最高分数的候选框重叠面积IOU大于一定的阈值，就将其删除。

    > Selective search算法：选择性搜索。用来生成候选区域
    >
    > - 计算区域级R里每个相邻区域的相似度S={s1,s2,....}
    > - while S≠∅：
    >   - 找出相似度最高的两个区域，将其合并为新集Rt，添加进R，
    >   - 从S中移除所有与上一步有关的子集
    >   - 重新计算Rt与所有自己的相似度（颜色，纹理，尺寸，交叠等）
    >
    > **摘抄**：选择性搜索算法使用《Efficient Graph-Based Image Segmentation》论文里的方法产生**初始的分割区域作为输入**，通过下面的步骤进行合并：
    >
    > - **首先将所有分割区域的外框加到候选区域列表中**
    > - **基于相似度合并一些区域**
    > - **将合并后的分割区域作为一个整体，跳到步骤1**
    >
    > - **通过不停的迭代，候选区域列表中的区域越来越大。可以说，我们通过自底向下的方法创建了越来越大的候选区域。表示效果如下**
    >   ![ss.png](https://i.loli.net/2020/03/08/1bBW7zMXtvn8hgI.png)
    > - 相似度：
    >   选择性搜索算法如何计算两个区域的像素度的呢？
    >   主要是通过以下四个方面：颜色、纹理、大小和形状交叠
    >   最终的相似度是这四个值取不同的权重相加

  - 特征提取：RCNN抽取了一个4096维的特征向量，采用的是Alexnet,基于Caffe进行的开发,候选区域的输入图像大小不一，为了AlexNet兼容，采取暴力的方法，就是无视候选区域的大小和形状，同一变化到227*227,（进行膨胀处理，再其周围附加上16pix的边框。将AlexNet模型进行微调，将分类数从1000改为21，比如20个物体类别+1背景。去掉最后的全连接层

  - AlexNet的最后一层卷积输出层之后的：两个模型

    - 回归问题：预测出（x,y,w,h)四个参数的值，从而得到方框的位置（x,y 表示方框的左上角位置，w,h分别指方框的宽度和高度）用欧氏距离做损失函数，使用SGD做训练。
    - 分类问题：

  - 目标检测准确的指标：

    - 交并比IoU(intersection-over-union)真实目标检测框和预测目标目标检测框的交集和并集的比值，称之为交并比，一般阈值设置为0.5，大于0.5，则可认为预测的是正样本。

    - 平均准确率均值mAP：每一个类别都有recall和precision绘制一条曲线，那么AP就是曲线下的面积，而mAP是多个类别的AP的平均值，这个值【0,1】，越大越好，这个指标是目标检测算法最为重要的个

      

  - **详细算法步骤**：

    - **输入图像**
    - **利用ss提取2000左右的候选区域**
    - **为了使候选区域进行大小统一，并将区域输入到CNN，将CNN的fc7层输出作为特征（将这个特征存到硬盘）**
    - **将提取到的特征输入SVM进行分类**
    - **使用回归器进行修正候选框的位置**

  - 缺点：

    - 候选得到的2k个区域都要经过一次CNN，速度非常慢
    - 检测速度很慢，一张图片都需要47s

- SPP-Net:（ROI pooling)：
  ![SPP.png](https://i.loli.net/2020/01/30/cg4vyM5GFOhEUSt.png)

  - 首先最后需要特征去做目标分类（需要21维的特征），前一层是一层全连接层，全连接层的输入必须是固定大小的输入维度，但是RCNN是通过将输入到AlexNet的图片大小统一（这种操作会使得图片的信息缺失和变形，从而影响识别的精度），再输入到网络，最后得到一层全连接层的输出。而SSP-Net是将先进行AlexNet的训练，再将输出的结果进行spp-net处理（spatial pyramid pooling layer），再通过全连接层。也就是最后的卷积层和全连接层之间加入SPP层。
  - SPP具体的做法：在conv5层得到的特征图是256层，每一层都做一次SSP：先把每一个特征图分割成多个不同尺寸的网格，比如```4*4.2*2.1*1```等，然后每一个网格做max pooling 这样256层特征图就形成了```16*256,4*256,1*256```维*（21维）特征，输出为256×21的特征。他们连起来就形成了固定长度的特征向量，将这个向量输入到后面的全连接层。
  - spp-net的整体流程：
    - 输入一张图像
    - 利用ss算法提取特征
    - 候选区域缩放
    - 利用spp-net进行特征提取
    - 分类和回归
  - 单一尺寸的训练：图像裁剪为224*224，卷积最后输出到SPP，得到一个固定大小的输出
  - 多尺寸的训练：多了图像缩放至180*180，使用SPP，得到一个固定大小的输出。
  - 优点：空间金字塔可以实现任意尺寸输入，固定大小输出，降低了计算时间，
  - 缺点：
    - 训练分多步骤（需要SVM，额外的回归器）
    - 空间开销大

- Fast R-CNN：(Selective Search +CNN + ROI)：在spp-net 的基础上，将最后一个卷积层的spp  layer改为ROI pooling layer;另外提出多任务损失函数，将边框回归直接加入到网络中进行训练。

  - ROI pooling layer 层是spp-net的简化版，这里只采用一种规格大小的网格7*7进行池化。
  - 分类采用：softmax代替svm
  - 存在的问题：选择性搜索SS，要找出所有的候选框，非常耗时。

- Faster R-CNN：
  
  ![1.png](https://i.loli.net/2020/03/09/7BUctSulXM1wFIT.png)
  
  ![搜狗截图20200129003017.png](https://i.loli.net/2020/01/30/rjmnDuBhVNPM9KS.png)

(RPN + CNN + ROI)是一个基于深度学习对象检测的一个典型案列，该算法用一个快速神经网络代替了运算速度很慢的选择性搜索算法：通过插入区域提议网络（ RPN ），来预测来自特征的建议。 RPN 决定查看“哪里”，这样可以减少整个推理过程的计算量。RPN 快速且高效地扫描每一个位置，来评估在给定的区域内是否需要作进一步处理，其实现方式如下：通过输出 k 个边界框建议，每个边界框建议都有 2 个值——代表每个位置包含目标对象和不包含目标对象的概率。一旦我们有了区域建议，就直接将它们送入 Fast R-CNN 。 并且，我们还添加了一个池化层、一些全连接层、一个 softmax 分类层以及一个边界框回归器。
  总之，Faster R-CNN 的速度和准确度更高。值得注意的是，虽然以后的模型在提高检测速度方面做了很多工作，但很少有模型能够大幅度的超越 Faster R-CNN 。换句话说， Faster R-CNN 可能不是最简单或最快速的目标检测方法，但仍然是性能最好的方法之一。
  近年来，主要的目标检测算法已经转向更快、更高效的检测系统。这种趋势在 You Only Look Once（YOLO），Single Shot MultiBox Detector（SSD）和基于区域的全卷积网络（ R-FCN ）算法中尤为明显，这三种算法转向在整个图像上共享计算。因此，这三种算法和上述的3种造价较高的R-CNN 技术有所不同。

> 对于每个3×3的窗口，作者就以这个滑动窗口的中心点对应原始图片的中心点。然后作者假定，这个3×3的窗口，是从原始图片通过SPP池化得到，而这个池化的面积及比例，就是一个个anchors。换句话说，对于每个3x3窗口，作者假定它来自９种不同原始区域的池化，但是这些池化在原始图片中的中心点，都完全一样。这个中心点，就是刚刚提到的，**3x3窗口中心点所对应的原始图片中的中心点**。如此一来，在每个窗口位置，我们都可以根据不同的长宽比例，不同的面积的anchors，逆向推导出它所对应的原始图片的一个区域，这个区域的尺寸以及坐标，都是已知。而这个区域，就是我们想要的proposal.接下来，每个proposal我们只输出６个参数，每个proposal和ground truth进行比较得到的前景概率和背景概率（２个参数）对应图片上的cls_score，由于每个proposal和groundtruth的位置及尺寸上的差异从proposal通过平移缩放得到ground truth需要的４个平移缩放参数（对应图片上bbox_pred）
> ![2.jpg](https://i.loli.net/2020/03/09/Kx6W1JASCwY9Xdz.jpg)

## **anchors**

其中每行的４个值（x1,y1,x2,y2）代表矩形左上角和右下角点的坐标。９个矩形共有３种形状，3 scale with box areas分别是｛128128 256256 512*512｝和 3 aspect ratios分别是近似｛（1:1 1:2 2:1）｝，所以共９种矩形。实际上通过anchors就引入了检测中常用的多尺度方法
![4.png](https://i.loli.net/2020/03/09/BJUH4hpyXR7QFZ5.png)

  - 将RPN放在最后一个卷积层的后面，这样通过RPN来选择候选区域

  - RPN网络主要用来生成region proposals，首先生成一堆anchor box ,对其进行裁剪过滤后通过softmax判断anchor属于前景还是后景，这是一个二分类问题，同时另一只分支bounding box regression修正anchor box ，形成精确的proposal

  - RPN网络：（使用全卷积层网络来预测相对anchor boxes 的offsets和confidences，因为预测层是个卷积层，RPN在一个特征图上所预测所有bounding boxes的offsets和直接预测坐标相比，预测offsets简化了问题，而且网络容易学习）
    - feature map （```60*40*512```）进入RPN之后先经过一层3x3的卷积，这样做的目的是进一步的集中特征信息，接着两个全卷积。上面的一层是对逐个元素对其9个Anchor box 进行二分类；下面一层逐个元素对其9个anchor box 四个坐标信息（这9个box的坐标是像素点的偏移量构成的）
    
    - anchors box的生成：先定义一个base——anchor，大小为16x16的box，在通过ratios=[0.5,1,2]（保持面积不变，长宽比改变）,scales=[8,16,32]（长宽都变为原来的几倍生成9个anchors。这样对于每一个feature map的像素点都有9个anchors 那么一共60x40x9=21600个anchor box。最后生成[21600,4]的数组坐标
    
    - RPN-data:这一层主要是为feature map上的60x40个像素点生成anchor box ，并对生成的box，进行过滤和编辑。ground truth是anchor中有目标的box
      - 去掉超过1000x600这原图的边界的anchor box
      - 如果anchor box与ground truth的IOU值>0.7,标记为正样本，
      - <0.3的标记为负样本
      - 其他的既不是正样本也不是负样本，标记为label=-1
      
    - rpn_loss_cls,rpn_loss_bbox,rpn_cls_prob
      - rpn_loss_cls:softmax分类类别概率
      - rpn_loss_bbox:smooth l1损失函数
      - rpn_cls_prob:计算概率值
      
    - 分类和回归的损失函数：
    
      ------
  
    #### RPN
  
  - softmax判定foreground与background:一副MxN大小的矩阵送入Faster RCNN网络后，到RPN网络变为(M/16)x(N/16)，不妨设W=M/16，H=N/16。在进入reshape与softmax之前，先做了1x1卷积
    ![3.png](https://i.loli.net/2020/03/09/zkQ6CnLwb4YM25N.png)
  
    经过1x1的卷积的输出图像为WxHx18大小（18是9个anchor box 和2个概率值（背景概率和目标概率）的乘积），reshape操作是主要原因是caffe的`blob=[batch_size,channel,height,width]`所以需要增加一个维度，用来存储batch_size；之后通过softmax，再reshape成没有batch_size形式的数组。
  
  - ###### bounding box regression原理
  
    ![5.jpg](https://i.loli.net/2020/03/09/shZCXibMY7cgf3d.jpg)
  
    绿色的为Ground Truth(GT),红色为提取得到的foreground anchors，那么即使 红色的框被识别器识别为飞机，但是由于红色的框定位不准，这张图相当于没有正确的检测出飞机，那么我们希望采用一种方法对红色的框进行微调，使得预测框和真值框的位置更接近，
  
    ![6.jpg](https://i.loli.net/2020/03/09/lnfTaSPmK75eqMg.jpg)
  
    对于窗口我们一般用四维向量（x,y,w,h）表示，分别表示窗口的中心点坐标和宽高，如下图，红色的框A代表原始的Foreground Anchors,绿色的框G代表目标的GT，我们的目标是寻找一种关系，使得输入原始的anchor A经过映射得到一个跟真实窗口G更接近的回归窗口G’，即给定anchorA=(Ax,Ay,Aw,Ah),GT=[Gx,Gy,Gw,Gh],寻找一种变换F:使得F(Ax,Ay,Aw,Ah)=(G’x,G’y,G’w,G’h),其中（G’x,G’y,G’w,G’h）≈(Gx, Gy, Gw, Gh)。
  
    通过变换：先平移再做缩放。
  
    proposal layer有３个输入：fg/bg anchors分类器结果rpn_prob_reshape，对应的bbox reg的[dx(A),dy(A),dw(A),dh(A)]变换量rpn_bbox_ped,以及im_info,另外还有参数feat_stride=16,这和上图对应
  
    首先解释下im_info，对于一幅任意大小的PQ图像，传入Fsater Rcnn前首先reshape到MN大小，im_info=[M, N, scale_factor]则保存了此次缩放的所有信息。然后经过Conv Layers，经过4次pooling变为WxH=(M/16)x(N/16)大小，其中feature_stride=16则保存了该信息，用于计算anchor偏移量。
    
  
  > ROI Pooling:
  >
  > - 将每一个region proposal图像划分为7x7，这样在取每个格子的最大值，也就是max pooling ，这样就能把不同大小的region proposal 划到统一大小的输出：
  > - 优点：
  >   - 显著加速训练和测试的速度
  
- SSD：single shot multibox detector,提取不同尺寸的特征图来做检测，大尺寸特征图可以用来检测小物体，而小尺寸的来检测大物体，采用不同尺寸和长宽比的先验框pior boxes,

  - 采用多尺度特征图用于检测：比较大的特征图能检测到比较小的物体，而比较小的特征图能检测到比较大的物体

  - 采用卷积进行检测：采用卷积的方式来得到检测值

  - 设置先验框：对于每个像素设置4个不同尺寸的先验框

  - 每一层网络的feature map的每一个cell，也就是像素点能产生不同数量的default box，一共卷积输出层的网络有6层，每一层卷积网络的每一个像素点产生的default box的数目是确定的{4，6，6，6，4，4}这几个数字是怎么计算得到的呢？

    > ```python
    > # 源代码中给出的形式
    > aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]] 
    > #但是论文中给出的是，这些尺寸是计算出来的，先计算出default box的大小，再算出宽高比
    > ar ∈{1, 2, 3,1/2,1/3}
    > ```
    >
    > 这更aspec_ratios有关系：在SSD中6层卷积层的每一个特征图的每一个中心点会产生2个大小不同的正方形default box，另外每设置一个aspect_ratio就增加两个长方形的default box，而文中aspect-ratio个数分别是1,2,2,2,1,1，所以会参数{4,6,6,6,4,4}不通数目的default box。

  - 下面在计算6层卷积层的default box的大小是怎么计算出来的：

    - 这是原作者的源代码：

      ``` python
      #coding:utf-8
      import math
       
      min_dim = 300   #######维度
      # conv4_3 ==> 38 x 38
      # fc7 ==> 19 x 19
      # conv6_2 ==> 10 x 10
      # conv7_2 ==> 5 x 5
      # conv8_2 ==> 3 x 3
      # conv9_2 ==> 1 x 1
      mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2'] #####prior_box来源层，可以更改。很多改进都是基于此处的调整。
      # in percent %
      min_ratio = 20 ####这里即是论文中所说的Smin=0.2，Smax=0.9的初始值，经过下面的运算即可得到min_sizes，max_sizes。具体如何计算以及两者代表什么，请关注我的博客SSD详解。这里产生很多改进。
      max_ratio = 90
      ####math.floor()函数表示：求一个最接近它的整数，它的值小于或等于这个浮点数。
      step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))####取一个间距步长，即在下面for循环给ratio取值时起一个间距作用。可以用一个具体的数值代替，这里等于17。
      min_sizes = []  ###经过以下运算得到min_sizes和max_sizes。
      max_sizes = []
      for ratio in range(min_ratio, max_ratio + 1, step):  ####从min_ratio至max_ratio+1每隔step=17取一个值赋值给ratio。注意range函数的作用。
      ########min_sizes.append（）函数即把括号内部每次得到的值依次给了min_sizes。
        min_sizes.append(min_dim * ratio / 100.)
        print(min_sizes)
        max_sizes.append(min_dim * (ratio + step) / 100.)
      min_sizes = [min_dim * 10 / 100.] + min_sizes
      max_sizes = [min_dim * 20 / 100.] + max_sizes
      steps = [8, 16, 32, 64, 100, 300]  ###这一步要仔细理解，即计算卷积层产生的prior_box距离原图的步长，先验框中心点的坐标会乘以step，相当于从feature map位置映射回原图位置，比如conv4_3输出特征图大小为38*38，而输入的图片为300*300，所以38*8约等于300，所以映射步长为8。这是针对300*300的训练图片。
      aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]
      print("*")
      print(min_sizes)
      print("**")
      print(max_sizes)
      
      ###output:
      [60.0]
      [60.0, 111.0]
      [60.0, 111.0, 162.0]
      [60.0, 111.0, 162.0, 213.0]
      [60.0, 111.0, 162.0, 213.0, 264.0]
      *
      [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]
      **
      [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]
      
      
      #有点疑惑：为什么min-sizes,的最小不是从循环中计算出的60开始的，而是从30开始的？
      '''为什么先计算了fc7层往后的，最后再计算conv4_3，因为conv4_3的计算是比较特殊的。虽然比率计算结果为20,37,54,71,88,105。看这些数据，其实是每一层的最大比率，也就是说在最后一层conv11_2中，min_size = 300*88/100.=264, max_size = 300*105/100.=315。所以在第一层conv4_3中，max_size=300*20/100.=60，但是min_size怎么计算呢？按照正常步骤走应该ratio为20-17=3，但是3太小了，所以作者将第一层的min_ratio设定了10，所以conv4_3的min_size= 300 * 10 /100. = 30，'''
      ```

      - 接下来计算default box 的大小：对于产生的正方形的default box ，一大一小，边长计算公式:小边长=min_size,大边长=sqrt(min_size*max_size);对于长方形default box ，height=1/sqrt(aspect_ratio)*min_size,   width=sqrt(ascept_ratio)*min_size;另一个长方形翻转就得到了，他们的面积是一样的。

  - 在匹配之后，使用default box的最高置信度对他们进行排序，使得正负样本之间的比率最多是3:1，以代替使用的所有负样本

  - 数据增强：为了使模型对于各种输入对象具有更好的鲁棒性，每一个训练图像通过一下三种方法之一随机采样：

    - 水平翻转，随机裁剪加颜色扭曲
    - 采取一个片段，使IOU值的为0.1,0.3,0.5,0.7,0.9
    - 随机采样一个片段 

  - prior box 是能够和ground truth box相匹配的box，

    > **先验框匹配**
    > 在训练过程中，首先要确定训练图片中的ground truth（真实目标）与哪个先验框来进行匹配，与之匹配的先验框所对应的边界框将负责预测它。在Yolo中，ground truth的中心落在哪个单元格，该单元格中与其IOU最大的边界框负责预测它。但是在SSD中却完全不一样，SSD的先验框与ground truth的匹配原则主要有两点。首先，对于图片中每个ground truth，找到与其IOU最大的先验框，该先验框与其匹配，这样，可以保证每个ground truth一定与某个先验框匹配。通常称与ground truth匹配的先验框为正样本（其实应该是先验框对应的预测box，不过由于是一一对应的就这样称呼了），反之，若一个先验框没有与任何ground truth进行匹配，那么该先验框只能与背景匹配，就是负样本。一个图片中ground truth是非常少的， 而先验框却很多，如果仅按第一个原则匹配，很多先验框会是负样本，正负样本极其不平衡，所以需要第二个原则。第二个原则是：对于剩余的未匹配先验框，若某个ground truth的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D) 大于某个阈值（一般是0.5），那么该先验框也与这个ground truth进行匹配。这意味着某个ground truth可能与多个先验框匹配，这是可以的。但是反过来却不可以，因为一个先验框只能匹配一个ground truth，如果多个ground truth与某个先验框 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D) 大于阈值，那么先验框只与IOU最大的那个先验框进行匹配。第二个原则一定在第一个原则之后进行，仔细考虑一下这种情况，如果某个ground truth所对应最大 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D) 小于阈值，并且所匹配的先验框却与另外一个ground truth的 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D) 大于阈值，那么该先验框应该匹配谁，答案应该是前者，首先要确保某个ground truth一定有一个先验框与之匹配。但是，这种情况我觉得基本上是不存在的。由于先验框很多，某个ground truth的最大 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctext%7BIOU%7D) 肯定大于阈值，所以可能只实施第二个原则既可以了。
    >
    > 尽管一个ground truth可以与多个先验框匹配，但是ground truth相对先验框还是太少了，所以负样本相对正样本会很多。为了保证正负样本尽量平衡，SSD采用了hard negative mining，就是对负样本进行抽样，抽样时按照置信度误差（预测背景的置信度越小，误差越大）进行降序排列，选取误差的较大的top-k作为训练的负样本，以保证正负样本比例接近1:3。

  - 损失函数：位置误差和置信度误差
    ![损失.png](https://i.loli.net/2020/02/01/ktGCzAHW62K1RXh.png)

    ![顺势2.png](https://i.loli.net/2020/02/01/P1S5p8XjlUEMrue.png)

  - 预测过程：对于每一个预测框，首先根据类别置信度确定其类别（置信度最大值）与置信度，并过滤掉属于背景的预测框，然后根据置信度阈值过滤掉阈值较低的预测框，对留下的预测框进行解码，根据先验框得到其真实的未知参数（解码后一般还需要做clip，防止预测框位置超出图片），解码之后，一般需要根据置信度进行降序排列，然后仅保留top-k（如400）个预测框。最后就是进行NMS算法，过滤掉那些重叠度较大的预测框。最后剩余的预测框就是检测结果了。

  - 性能评估：

  - 数据增强技术很重要，对mAP的提升很大

  - 使用不同长宽比的先验框可以得到更好的结果

  - 采用多尺度的特征图用于检测也是很重要的

- YOLO V1：

  - YOLO训练和检测均是在一个单独网络中进行，没有显示求取region proposal的过程

  - 优点：

    - 检测物体非常快，没有复杂的检测过程，只是对输入图像做回归，
    - 可以很好的避免背景错误
    - 可以学到物体的泛化特征

  - 缺点

    - 精度低于faster RCNN ，SSD等
    - 容易产生定位错误
    - 对小物体的检测效果不好，尤其是对密集的小物体美英伟一个网格只能预测2个物体

  - 过程

    - 将一整图像分割成7×7(S×S)的网格，对每一个网格设置两个bounding box (B)，每一个bb都有{x,y,w,h,confidence置信度}

      > confidence:是针对每一个bb的，这个置信度说明是否含有物体，以及这个box的坐标预测的有多准

    - condition class probability (C)条件类别概率，对于每一个网格还要计算所有类别的所属概率，一共要识别多少个类，就有多少个值

      > 最终一张图片的输出为：S×S×（B×5+C)           7×7×30张量

  - 网络设计：

    - 24个conv和2个FC层
      ![YOLO1.png](https://i.loli.net/2020/02/02/p9JDwBIeZAz34Tq.png)

  - 训练：首先利用ImageNet 1000-class的分类任务数据集做预训练卷积层，使用Google net的前20层卷积层，加上一个average-pooling layer 最后加上一个全连接层，作为预训练的网络，训练大约一周的时间，使得在ImageNet 2012 的验证数据集top-5的精度达到88%，这个结果更Googlenet的效果相当。将Pretrain的结果的前20层卷积层应用到Detection中，并加入剩下的4个卷积层及2个全连接。同时为了获取更精细化的结果，将输入图像的分辨率由 224* 224 提升到 448* 448。将所有的预测结果都归一化到 0~1, 使用 Leaky RELU 作为激活函数。

    > 损失函数的设计 目标就是让坐标（x,y,w,h,c)以及C得到很好的平衡，简单的全部采用平方差损失函数，这样有不足：
    >
    > - 8维的坐标损失和20维的分类损失设为同等重要显然是不合理的
    > - 如果一些网格中没有object，就将这些网格的bb的置信度设置为0，相比于较少有目标的网格，这些不包含目标的网格对梯度更新的贡献会远大于包含目标的网格对梯度的影响，这会导致网络不稳定甚至发散，为了解决这些问题，YOLO 的损失函数修改为：
    >   

    ![loss.png](https://i.loli.net/2020/02/02/SVjHeg5pIPKbhvx.png)

    - 对上述![ij.png](https://i.loli.net/2020/02/02/UEWX7sau3rHIYSf.png)

    表示第i个网格中的第j个bb是否负责这个object，两个bb与真值box求取IOU，选取大的就是对object负责的

    - 更重视8维的坐标预测，给这些损失前面赋予更大的loss weight ，记做 λcoord,在Pascal VOC训练中取值5，对没有目标框的bb赋予小的loss记做 λnoobj，在Pascal VOC训练取值0.5,；有目标的bb和类别loss，权值去1.
    - 对不同大小的bbox预测中，相比于大bbox预测偏一点，小box预测偏相同的尺寸对IOU的影响更大。而sum-square error loss中对同样的偏移loss是一样。
      为了缓和这个问题，作者用了一个巧妙的办法，就是将box的width和height取平方根代替原本的height和width。 如下图：small bbox的横轴值较小，发生偏移时，反应到y轴上的loss（下图绿色）比big box(下图红色)要大。

- YOLO V2：

  - 改进
    - 采用新的网络结构draknet19
    - 引入anchor box 的思想
    - 输出层：卷积层代替YOLO V1的全连接层
    - 联合使用coco和ImageNet物体分类标注数据
    - 识别种类，精度，速度和定位准确性都有很大的提升
  - 详细改进：
    - batch normalization：在没一次的3×3的卷积之后，都添加BN，对模型的泛化能力有所帮助，去掉dropout的模型依旧没有过拟合
    - 高分辨率分类器：把图像resize到256×256下，会丢失一些小物体的信息，YOLOv2直接使用448×448的分辨率来fine-tune分类网络，使用高分辨率的分类网络提升将近4%的mAP值
    - anchor boxes：YOLO v2采用的是anchor box是从训练集中训练得到的5种大小的box来做bounding box的预测。去掉全连接层，使用anchor boxes来预测bb;去掉一个池化层，使得卷积层的输出有更高的分辨率；将输入图像的尺寸调整到416×416，这样的特征图的输出就是一个奇数（池化的总步长是2**5=32，所以最后的输出就是416/32=13），有一个中心栅格。作者观察到，很多的物体，尤其是简答的物体往往会位于图像的中心，有一个中心栅格的话就可以用这个中心栅格专门去负责预测这些中西落在图像中心附近的物体，而不需要图像中心附近的4个栅格去预测这些物体。
      - YOLO v1是通过栅格去预测属于某个类的概率，有一组概率（S×S((B×5+C))；而YOLO v2是由bounding boxes去预测，所有B组条件类别概率(S×S(B+(5+C))
      - YOLO v2 中的anchor boxes是通过k-means在训练集中学到的，而不是使用欧氏距离会让大的bb比小的bb产生更多的error，为此新定义的距离公式（这里聚类之间的距离是通过计算任意两个框的IOU得到的：d(box,centroid)=1-IOU(box,centroid)。最后根据测试的结果选出K=5作为anchor boxes的常用框。
      - 候选框的预测：
        ![v2.png](https://i.loli.net/2020/02/04/rIZOlTaB6EkuoQn.png)
    - 细粒度特征：为了识别细粒度的特征，YOLO V2采用通过添加passthrough layer，将前一个卷积快的26×26的分辨率的特征图的信息融合起来
    - multi-scale training：修改输入不同尺寸的图像

- 语义分割：

  - FCN：全卷积网络，对图像进行像素级的分类，从而解决语义级别的图像分割问题。最后输出的特征图的大小和原输入图像的大小一致，所以这样对每一个像素预测。

    - 核心思想：

      - 不含全连接层的全卷积网络，可适应任意尺寸输入。
      - 增大最后一层的特征图尺寸采用的是反卷积层（也叫转置卷积，是一种上采样的方式，增大特征图的大小）能够输出和输入一样大的图像
      - 结合不同的深度结果的跳级结构，同时确保鲁棒性和精确性。

    - CNN和FCN的比较，如下图
      ![1.png](https://i.loli.net/2020/02/11/E7eOdKpSiTtRAh2.png)

      上面的CNN最后输出是分类类别，是一个一维的张量（1×1000，表示1000个类别概率），而下面的FCN全部是卷积层和池化层的结合，最后得到的是二维的张量（1×1×1000表示最后输出的是一个表示预测图像中的1000个类别的特征图）

      > 输入可以使任意尺寸，输出与输入尺寸的大小相同。如果在PASCAL VOC数据集训练的，那么
      >
      > 最后输出的深度就是21（20类别+1背景）；COCO数据及上的是80，ImageNet上是1000.

    - 网络结构：
      ![2.png](https://i.loli.net/2020/02/11/qgEFue2WrafYJch.png)

      - 输入图像的大小是500×500×3,13层conv和5层maxpooling，这个是VGG16的前5层网络，去掉了最后的几层全连接网络。

      - 最后一层的池化层输出特征图的大小是22×22×512，6,7层是16×16×4096,8层是16×16×21的feature map。

      - 先将8层的输出上采样到34×34×21再和4层的特征图结合结合在一起。再通过上采样到70×70×21

      - 将上一步的输出和3层的特征图结合在一起，再通过上采样得到和原图像一样大特征图。
        ![3.png](https://i.loli.net/2020/02/11/sQGJeAE6cWPXNhD.png)

        > 为什么不直接在第8层的卷积之后之间将输出特征图直接上采样到原输入图像大小呢？因为这样得到的结果不够精细，细节部分无法恢复，所以才采用上述的跳级式的特征结合，使得最后的输出结果更准确。
        > ![4.png](https://i.loli.net/2020/02/11/3vduFJNLjAnlp4z.png)

    - 接下来得到了500×500×21的特征图，怎么具体的逐像素点预测分类：一层特征图表示一种类别特征图，如果原输入图像中没有其中的一类，那么最后输出的21张特征图中对应的fm,所有的值就是0；
      **对逐个像素求在21张图像该像素位置的最大值数值就作为该像素点的分类**

    - 训练：分阶段训练：

      - 第一阶段训练主干网络的，也就是分类网络去掉全连接层进行训练
      - 第二阶段：小特征图上采样32s
      - 第三阶段：上一层加上第四层的fm进行训练，上采样到16s
      - 第四阶段：上一层加上第三层的fm进行训练,上采样到8s,这次得到的效果是最高的。

    - **FCN 的缺点**：

      1. 分割的结果不够精细。图像过于模糊或平滑，没有分割出目标图像的细节
      2. 因为模型是基于CNN改进而来，即便是用卷积替换了全连接，但是依然是独立像素进行分类，没有充分考虑像素与像素之间的关系
      3. 也是因为每个模型都有自己的不足，所以才会出现各式各样的模型来解决它们的问题。大家如果能分析出每个模型的缺点，那我们也可以构建新的模型去完善他们的模型，这其实就是创新的过程

- 实例分割：机器自动从图像中用目标检测方法框出不同实例，再用语义分割方法在不同实例区域内进行逐像素标记。

  - Mask-RCNN：通用的实例分割架构，以Faster RCNN为原型，增加一个分支用于分割任务，比FasterRCNN要慢，可用于人的姿态估计等其他任务。
    ![5.png](https://i.loli.net/2020/02/11/rdyT5XpvCPwB3eN.png)

    - 对每一个proposal box 都要进行FCN进行语义分割，分割任务与定位，分类任务是同时进行的。
    - ROI align 代替了ROI pooling，因为RoI Pooling并不是按照像素一一对齐的（pixel-to-pixel alignment），也许这对bbox的影响不是很大，但对于mask的精度却有很大影响。使用RoI Align后mask的精度从10%显著提高到50%
    - 引入语义分割分支，实现了mask和class预测的关系的解耦，mask分支只做语义分割，类型预测的任务交给另一个分支，这与原本的FCN是不同的，原来的FCN在预测的同时还要预测mask所属的种类

  - 基本结构：与Faster RCNN采用相同的two-state步骤：首先找到RPN，然后对找到的每一个ROI进行分类和回归，并找到二进制 mask

  - 损失函数：L=L分类+L回归+L分割

  - RoIAlign:
    为了解决ROI Pooling的问题，曲线量化操作，使用双线性内插的方法获得坐标为浮点数的像素点上的图像数值，

    - 遍历每一个候选区域，保持浮点数边界不做量化。

    - 将候选区域分割成k x k个单元，每个单元的边界也不做量化。

    - 在每个单元中计算固定四个坐标位置，用双线性内插的方法计算出这四个位置的值，然后进行最大池化操作。

    - 这里对上述步骤的第三点作一些说明：这个固定位置是指在每一个矩形单元（bin）中按照固定规则确定的位置。比如，如果采样点数是1，那么就是这个单元的中心点。如果**采样点**数是4，那么就是把这个单元平均分割成四个小方块以后它们分别的中心点。显然这些采样点的坐标通常是浮点数，所以需要使用插值的方法得到它的像素值。在相关实验中，作者发现将采样点设为4会获得最佳性能，甚至直接设为1在性能上也相差无几。事实上，ROI Align 在遍历取样点的数量上没有ROIPooling那么多，但却可以获得更好的性能，这主要归功于解决了misalignment的问题。值得一提的是，我在实验时发现，ROI Align在VOC2007数据集上的提升效果并不如在COCO上明显。经过分析，造成这种区别的原因是COCO上小目标的数量更多，而小目标受misalignment问题的影响更大（比如，同样是0.5个像素点的偏差，对于较大的目标而言显得微不足道，但是对于小目标，误差的影响就要高很多）。

      ![7.png](https://i.loli.net/2020/02/11/ZXfobREqxAH4lgI.png)上图为图（3），ROI Align层要将feature map固定为2*2大小，那些蓝色的点即为采样点，然后每个bin中有4个采样点，则这四个采样点经过MAX得到ROI output；

      它的优点：通过双线性插值避免了量化操作，保存了原始ROI的空间分布，有效避免了误差的产生；

    > ROI Pooling:
    > 这一层的功能是将不同大小的ROI区域映射到固定大小的特征图上。
    >
    > 缺点是：由于两次量化带来的误差：
    >
    > - 将候选框边界量化为整数点坐标的坐标值
    > - 将量化后的边界区域平均分割成k×k个单元，对每一个单元边界进行量化
    >   ![6.png](https://i.loli.net/2020/02/11/A8tVp24ycZGNj39.png)

- 图像风格迁移 image style transfer using convolutional netural networks

  - 需要待转换图像A和风格图像（下面小的图像），可以采用不同的主干网络来进行特征提取，进而生成具有风格图像的生成图像B。
    ![1.png](https://i.loli.net/2020/02/12/kzCusmQr5jPcN3d.png)

  - 一般在特征提取之前都会将图像resize到相同的尺寸，另外网络采用的是VGG19的网络来提取特征

    ![2.png](https://i.loli.net/2020/02/12/Tc89GEDZReihn2L.png)

> 上面的图通过VGG19的网络，对待转换的图像和风格图像来输入，得到的不同层的feature map的可以从上面的看出一些信息，
>
> - 对于下面的content reconstructions内容重构：到VGG19的conv4层的特征图保留了较多的原图像特征，但是conv5比较模糊，丢失了很多的原图像特征。所以才在后面算法中，只采用生成的特征图做Loss；另一方面得到的结论是低层特征一般表达输入图像的像素信息（或者是小物体），而高层特征一般是输入图像的物体和布局体征（大物体）
>
> - 对于style reconstructions可以看出，不同层的特征表达有不同的视觉效果，因此在后面提取style feature map的时候采用的是多层特征融合，这样风格表达会更加丰富。
>   ![5.png](https://i.loli.net/2020/02/12/jP5LMmTGlxniU3k.png)
>
>   上图是采用低层特征和高层特征的效果对比，可以看出低层特征会保留更多的原图细节信息。

- 网络结构图
  ![3.png](https://i.loli.net/2020/02/12/2nClrmod7E49YPG.png)

  - 上图有主要有三部分组成：

    - 左边的：输入是风格图像，网络是VGG19，每一层的特征图![13.png](https://i.loli.net/2020/02/12/x3tf4FmpXalsLqC.png)都要和中间的特征图![8.png](https://i.loli.net/2020/02/12/SuZL9QYwpO63PD4.png)做运算

    - 中间的：x是随机生成的白噪声图像，网络是VGG19这里的![7.png](https://i.loli.net/2020/02/12/JfgDyH2Mvu1aBZe.png)是中间网络生成的特征图，在通过Gram矩阵得到![8.png](https://i.loli.net/2020/02/12/SuZL9QYwpO63PD4.png)公式是![9.png](https://i.loli.net/2020/02/12/cJjKiZXflSB2rEv.png)

      > 上面的l表示网络的第几层，i表示第几个feature map，j也表示第几个feature map ,k表示feature map中的第k个元素。因此这个公式就是对两个feature map 求内积。那为什么会引入Gram矩阵呢？其实就是为了表达图像的纹理特征

    - 右边的：输入是原始图像，也就是待转换的图像，网也是VGG19，但是这个只输出conv4的特征和中间的网络的conv4做loss。

  - 损失函数：

    - 左边的和中间的损失：
      ![10.png](https://i.loli.net/2020/02/12/Eepj2T3z5HvDX4M.png)

      ![11.png](https://i.loli.net/2020/02/12/Kx6SMehsPtnZ9EH.png)
      w权值用来表达各层特征的重要性，这个损失就是用来描述风格的差异。

    - 右边的和中间的损失：

      ![15.png](https://i.loli.net/2020/02/12/mSoKyJAxsjkagf7.png)

    - 总的损失：
      ![12.png](https://i.loli.net/2020/02/12/Owu6qhoWdMNKiJs.png)

      采用梯度下降更新参数x

      > 关于初始化的比较。在文中（Figure2）是采用随机生成一个白噪声图像来初始化的，在文中作者也对比了其他初始化方法，比如用style image初始化或用content image进行初始化。结果表明不同的初始化方式对最终的结果没有太大的影响。每次随机初始化都会生成不同的结果，如Figure6中的C有4张。但是用style image或content image初始化生成的结果是固定的。
      > ![6.png](https://i.loli.net/2020/02/12/26uSVPnHsh9FGb1.png)

- 人脸识别
  - 人脸检测
  - 人脸配准：定位出人脸五官关键点坐标的一项技术。
  - 人脸属性分析：
  - 人脸验证/比对：判断两个人脸是否是同一个人
  - 人脸识别；输出人脸特征和注册数据库中的N个身份进行逐个对比，找出相似度最大的。
  - 人脸检索：
  - 人脸聚类：根据相似度进行分析，将属于同一身份的人划分到同一区域。

